#!/usr/bin/env python3\n\"\"\"\nç³»ç»Ÿé›†æˆå’Œæœ€ç»ˆæµ‹è¯•è¿è¡Œè„šæœ¬\n\"\"\"\n\nimport asyncio\nimport sys\nimport os\nimport time\nimport subprocess\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional\n\n# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\nfrom quant_framework.monitoring.logger import get_logger\nfrom quant_framework.core.config import get_settings\n\nlogger = get_logger(__name__)\n\n\nclass SystemTestRunner:\n    \"\"\"ç³»ç»Ÿæµ‹è¯•è¿è¡Œå™¨\"\"\"\n    \n    def __init__(self):\n        self.settings = get_settings()\n        self.test_results = {\n            'start_time': None,\n            'end_time': None,\n            'total_duration': 0,\n            'tests': {},\n            'summary': {\n                'total': 0,\n                'passed': 0,\n                'failed': 0,\n                'skipped': 0,\n                'errors': []\n            }\n        }\n    \n    async def run_all_tests(self) -> Dict[str, Any]:\n        \"\"\"è¿è¡Œæ‰€æœ‰ç³»ç»Ÿæµ‹è¯•\"\"\"\n        logger.info(\"å¼€å§‹ç³»ç»Ÿé›†æˆå’Œæœ€ç»ˆæµ‹è¯•\")\n        self.test_results['start_time'] = datetime.now().isoformat()\n        \n        test_suites = [\n            ('unit_tests', 'å•å…ƒæµ‹è¯•', self.run_unit_tests),\n            ('integration_tests', 'é›†æˆæµ‹è¯•', self.run_integration_tests),\n            ('system_integration', 'ç³»ç»Ÿé›†æˆæµ‹è¯•', self.run_system_integration_tests),\n            ('jqdata_migration', 'èšå®½è¿ç§»æµ‹è¯•', self.run_jqdata_migration_tests),\n            ('performance_tests', 'æ€§èƒ½æµ‹è¯•', self.run_performance_tests),\n            ('stress_tests', 'å‹åŠ›æµ‹è¯•', self.run_stress_tests),\n            ('stability_tests', 'ç¨³å®šæ€§æµ‹è¯•', self.run_stability_tests),\n            ('security_tests', 'å®‰å…¨æµ‹è¯•', self.run_security_tests),\n            ('end_to_end_tests', 'ç«¯åˆ°ç«¯æµ‹è¯•', self.run_end_to_end_tests)\n        ]\n        \n        for test_id, test_name, test_func in test_suites:\n            logger.info(f\"å¼€å§‹æ‰§è¡Œ: {test_name}\")\n            \n            try:\n                start_time = time.time()\n                result = await test_func()\n                duration = time.time() - start_time\n                \n                self.test_results['tests'][test_id] = {\n                    'name': test_name,\n                    'status': 'passed' if result['success'] else 'failed',\n                    'duration': duration,\n                    'details': result,\n                    'timestamp': datetime.now().isoformat()\n                }\n                \n                if result['success']:\n                    self.test_results['summary']['passed'] += 1\n                    logger.info(f\"âœ… {test_name} é€šè¿‡ ({duration:.2f}s)\")\n                else:\n                    self.test_results['summary']['failed'] += 1\n                    logger.error(f\"âŒ {test_name} å¤±è´¥ ({duration:.2f}s)\")\n                    self.test_results['summary']['errors'].extend(result.get('errors', []))\n                \n            except Exception as e:\n                self.test_results['tests'][test_id] = {\n                    'name': test_name,\n                    'status': 'error',\n                    'duration': 0,\n                    'error': str(e),\n                    'timestamp': datetime.now().isoformat()\n                }\n                \n                self.test_results['summary']['failed'] += 1\n                self.test_results['summary']['errors'].append(f\"{test_name}: {str(e)}\")\n                logger.error(f\"ğŸ’¥ {test_name} å‡ºé”™: {e}\")\n            \n            self.test_results['summary']['total'] += 1\n        \n        self.test_results['end_time'] = datetime.now().isoformat()\n        self.test_results['total_duration'] = sum(\n            test['duration'] for test in self.test_results['tests'].values()\n        )\n        \n        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š\n        await self.generate_test_report()\n        \n        return self.test_results\n    \n    async def run_unit_tests(self) -> Dict[str, Any]:\n        \"\"\"è¿è¡Œå•å…ƒæµ‹è¯•\"\"\"\n        try:\n            result = subprocess.run([\n                'python', '-m', 'pytest',\n                'tests/unit/',\n                '-v',\n                '--tb=short',\n                '--json-report',\n                '--json-report-file=test_reports/unit_tests.json'\n            ], capture_output=True, text=True, cwd=project_root)\n            \n            return {\n                'success': result.returncode == 0,\n                'stdout': result.stdout,\n                'stderr': result.stderr,\n                'return_code': result.returncode\n            }\n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    async def run_integration_tests(self) -> Dict[str, Any]:\n        \"\"\"è¿è¡Œé›†æˆæµ‹è¯•\"\"\"\n        try:\n            result = subprocess.run([\n                'python', '-m', 'pytest',\n                'tests/integration/',\n                '-v',\n                '--tb=short',\n                '--json-report',\n                '--json-report-file=test_reports/integration_tests.json'\n            ], capture_output=True, text=True, cwd=project_root)\n            \n            return {\n                'success': result.returncode == 0,\n                'stdout': result.stdout,\n                'stderr': result.stderr,\n                'return_code': result.returncode\n            }\n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    async def run_system_integration_tests(self) -> Dict[str, Any]:\n        \"\"\"è¿è¡Œç³»ç»Ÿé›†æˆæµ‹è¯•\"\"\"\n        try:\n            result = subprocess.run([\n                'python', '-m', 'pytest',\n                'tests/system/test_system_integration.py',\n                '-v',\n                '--tb=short',\n                '--json-report',\n                '--json-report-file=test_reports/system_integration.json'\n            ], capture_output=True, text=True, cwd=project_root)\n            \n            return {\n                'success': result.returncode == 0,\n                'stdout': result.stdout,\n                'stderr': result.stderr,\n                'return_code': result.returncode\n            }\n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    async def run_jqdata_migration_tests(self) -> Dict[str, Any]:\n        \"\"\"è¿è¡Œèšå®½è¿ç§»æµ‹è¯•\"\"\"\n        try:\n            result = subprocess.run([\n                'python', '-m', 'pytest',\n                'tests/system/test_jqdata_migration.py',\n                '-v',\n                '--tb=short',\n                '--json-report',\n                '--json-report-file=test_reports/jqdata_migration.json'\n            ], capture_output=True, text=True, cwd=project_root)\n            \n            return {\n                'success': result.returncode == 0,\n                'stdout': result.stdout,\n                'stderr': result.stderr,\n                'return_code': result.returncode\n            }\n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    async def run_performance_tests(self) -> Dict[str, Any]:\n        \"\"\"è¿è¡Œæ€§èƒ½æµ‹è¯•\"\"\"\n        try:\n            result = subprocess.run([\n                'python', '-m', 'pytest',\n                'tests/performance/',\n                '-v',\n                '--tb=short',\n                '--json-report',\n                '--json-report-file=test_reports/performance_tests.json'\n            ], capture_output=True, text=True, cwd=project_root)\n            \n            return {\n                'success': result.returncode == 0,\n                'stdout': result.stdout,\n                'stderr': result.stderr,\n                'return_code': result.returncode\n            }\n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    async def run_stress_tests(self) -> Dict[str, Any]:\n        \"\"\"è¿è¡Œå‹åŠ›æµ‹è¯•\"\"\"\n        try:\n            result = subprocess.run([\n                'python', '-m', 'pytest',\n                'tests/system/test_stress_testing.py',\n                '-v',\n                '-m', 'stress',\n                '--tb=short',\n                '--json-report',\n                '--json-report-file=test_reports/stress_tests.json'\n            ], capture_output=True, text=True, cwd=project_root)\n            \n            return {\n                'success': result.returncode == 0,\n                'stdout': result.stdout,\n                'stderr': result.stderr,\n                'return_code': result.returncode\n            }\n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    async def run_stability_tests(self) -> Dict[str, Any]:\n        \"\"\"è¿è¡Œç¨³å®šæ€§æµ‹è¯•\"\"\"\n        try:\n            # ç¨³å®šæ€§æµ‹è¯•é€šå¸¸éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†\n            result = subprocess.run([\n                'python', '-m', 'pytest',\n                'tests/system/test_stability.py',\n                '-v',\n                '--tb=short',\n                '--json-report',\n                '--json-report-file=test_reports/stability_tests.json'\n            ], capture_output=True, text=True, cwd=project_root)\n            \n            return {\n                'success': result.returncode == 0,\n                'stdout': result.stdout,\n                'stderr': result.stderr,\n                'return_code': result.returncode\n            }\n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    async def run_security_tests(self) -> Dict[str, Any]:\n        \"\"\"è¿è¡Œå®‰å…¨æµ‹è¯•\"\"\"\n        try:\n            # è¿è¡Œbanditå®‰å…¨æ‰«æ\n            bandit_result = subprocess.run([\n                'bandit', '-r', 'quant_framework/',\n                '-f', 'json',\n                '-o', 'test_reports/security_bandit.json'\n            ], capture_output=True, text=True, cwd=project_root)\n            \n            # è¿è¡Œsafetyä¾èµ–å®‰å…¨æ£€æŸ¥\n            safety_result = subprocess.run([\n                'safety', 'check',\n                '--json',\n                '--output', 'test_reports/security_safety.json'\n            ], capture_output=True, text=True, cwd=project_root)\n            \n            # å®‰å…¨æµ‹è¯•é€šè¿‡æ¡ä»¶ï¼šæ²¡æœ‰é«˜å±æ¼æ´\n            success = bandit_result.returncode in [0, 1] and safety_result.returncode == 0\n            \n            return {\n                'success': success,\n                'bandit': {\n                    'stdout': bandit_result.stdout,\n                    'stderr': bandit_result.stderr,\n                    'return_code': bandit_result.returncode\n                },\n                'safety': {\n                    'stdout': safety_result.stdout,\n                    'stderr': safety_result.stderr,\n                    'return_code': safety_result.returncode\n                }\n            }\n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    async def run_end_to_end_tests(self) -> Dict[str, Any]:\n        \"\"\"è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•\"\"\"\n        try:\n            # ç«¯åˆ°ç«¯æµ‹è¯•ï¼šå®Œæ•´çš„ç­–ç•¥åˆ›å»ºã€å›æµ‹ã€ç»“æœåˆ†ææµç¨‹\n            from tests.system.test_system_integration import TestSystemIntegration\n            \n            test_instance = TestSystemIntegration()\n            \n            # è¿è¡Œå…³é”®çš„ç«¯åˆ°ç«¯æµ‹è¯•\n            await test_instance.test_complete_strategy_lifecycle()\n            await test_instance.test_data_pipeline_integration()\n            await test_instance.test_jqdata_compatibility_integration()\n            \n            return {\n                'success': True,\n                'message': 'ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡'\n            }\n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    async def generate_test_report(self):\n        \"\"\"ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š\"\"\"\n        # ç¡®ä¿æŠ¥å‘Šç›®å½•å­˜åœ¨\n        report_dir = project_root / 'test_reports'\n        report_dir.mkdir(exist_ok=True)\n        \n        # ç”ŸæˆJSONæŠ¥å‘Š\n        json_report_path = report_dir / 'system_test_report.json'\n        with open(json_report_path, 'w', encoding='utf-8') as f:\n            json.dump(self.test_results, f, indent=2, ensure_ascii=False)\n        \n        # ç”ŸæˆHTMLæŠ¥å‘Š\n        html_report = self.generate_html_report()\n        html_report_path = report_dir / 'system_test_report.html'\n        with open(html_report_path, 'w', encoding='utf-8') as f:\n            f.write(html_report)\n        \n        logger.info(f\"æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ:\")\n        logger.info(f\"  - JSONæŠ¥å‘Š: {json_report_path}\")\n        logger.info(f\"  - HTMLæŠ¥å‘Š: {html_report_path}\")\n    \n    def generate_html_report(self) -> str:\n        \"\"\"ç”ŸæˆHTMLæµ‹è¯•æŠ¥å‘Š\"\"\"\n        summary = self.test_results['summary']\n        success_rate = (summary['passed'] / summary['total'] * 100) if summary['total'] > 0 else 0\n        \n        html = f\"\"\"\n<!DOCTYPE html>\n<html lang=\"zh-CN\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>é‡åŒ–æŠ•èµ„ç ”ç©¶æ¡†æ¶ - ç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š</title>\n    <style>\n        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}\n        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}\n        .header {{ text-align: center; margin-bottom: 30px; }}\n        .summary {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin-bottom: 30px; }}\n        .summary-card {{ background: #f8f9fa; padding: 20px; border-radius: 8px; text-align: center; }}\n        .summary-card h3 {{ margin: 0 0 10px 0; color: #333; }}\n        .summary-card .number {{ font-size: 2em; font-weight: bold; }}\n        .passed {{ color: #28a745; }}\n        .failed {{ color: #dc3545; }}\n        .total {{ color: #007bff; }}\n        .test-results {{ margin-top: 30px; }}\n        .test-item {{ background: white; border: 1px solid #ddd; border-radius: 8px; margin-bottom: 15px; overflow: hidden; }}\n        .test-header {{ padding: 15px; background: #f8f9fa; border-bottom: 1px solid #ddd; display: flex; justify-content: space-between; align-items: center; }}\n        .test-name {{ font-weight: bold; }}\n        .test-status {{ padding: 5px 10px; border-radius: 4px; color: white; font-size: 0.9em; }}\n        .status-passed {{ background: #28a745; }}\n        .status-failed {{ background: #dc3545; }}\n        .status-error {{ background: #6c757d; }}\n        .test-details {{ padding: 15px; display: none; }}\n        .test-details.show {{ display: block; }}\n        .progress-bar {{ width: 100%; height: 20px; background: #e9ecef; border-radius: 10px; overflow: hidden; margin: 20px 0; }}\n        .progress-fill {{ height: 100%; background: linear-gradient(90deg, #28a745, #20c997); transition: width 0.3s ease; }}\n        .errors {{ background: #f8d7da; border: 1px solid #f5c6cb; border-radius: 4px; padding: 15px; margin-top: 20px; }}\n        .errors h3 {{ color: #721c24; margin-top: 0; }}\n        .error-item {{ background: white; padding: 10px; margin: 5px 0; border-radius: 4px; font-family: monospace; font-size: 0.9em; }}\n    </style>\n    <script>\n        function toggleDetails(id) {{\n            const details = document.getElementById('details-' + id);\n            details.classList.toggle('show');\n        }}\n    </script>\n</head>\n<body>\n    <div class=\"container\">\n        <div class=\"header\">\n            <h1>é‡åŒ–æŠ•èµ„ç ”ç©¶æ¡†æ¶</h1>\n            <h2>ç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š</h2>\n            <p>ç”Ÿæˆæ—¶é—´: {self.test_results['end_time']}</p>\n        </div>\n        \n        <div class=\"summary\">\n            <div class=\"summary-card\">\n                <h3>æ€»æµ‹è¯•æ•°</h3>\n                <div class=\"number total\">{summary['total']}</div>\n            </div>\n            <div class=\"summary-card\">\n                <h3>é€šè¿‡æµ‹è¯•</h3>\n                <div class=\"number passed\">{summary['passed']}</div>\n            </div>\n            <div class=\"summary-card\">\n                <h3>å¤±è´¥æµ‹è¯•</h3>\n                <div class=\"number failed\">{summary['failed']}</div>\n            </div>\n            <div class=\"summary-card\">\n                <h3>æˆåŠŸç‡</h3>\n                <div class=\"number total\">{success_rate:.1f}%</div>\n            </div>\n            <div class=\"summary-card\">\n                <h3>æ€»è€—æ—¶</h3>\n                <div class=\"number total\">{self.test_results['total_duration']:.1f}s</div>\n            </div>\n        </div>\n        \n        <div class=\"progress-bar\">\n            <div class=\"progress-fill\" style=\"width: {success_rate}%\"></div>\n        </div>\n        \n        <div class=\"test-results\">\n            <h3>æµ‹è¯•è¯¦æƒ…</h3>\n\"\"\"\n        \n        for test_id, test_data in self.test_results['tests'].items():\n            status_class = f\"status-{test_data['status']}\"\n            html += f\"\"\"\n            <div class=\"test-item\">\n                <div class=\"test-header\" onclick=\"toggleDetails('{test_id}')\" style=\"cursor: pointer;\">\n                    <div class=\"test-name\">{test_data['name']}</div>\n                    <div>\n                        <span class=\"test-status {status_class}\">{test_data['status'].upper()}</span>\n                        <span style=\"margin-left: 10px; color: #666;\">{test_data['duration']:.2f}s</span>\n                    </div>\n                </div>\n                <div id=\"details-{test_id}\" class=\"test-details\">\n                    <p><strong>æ‰§è¡Œæ—¶é—´:</strong> {test_data['timestamp']}</p>\n                    <p><strong>è€—æ—¶:</strong> {test_data['duration']:.2f} ç§’</p>\n\"\"\"\n            \n            if 'error' in test_data:\n                html += f\"<p><strong>é”™è¯¯:</strong> <code>{test_data['error']}</code></p>\"\n            \n            if 'details' in test_data and test_data['details']:\n                html += \"<p><strong>è¯¦ç»†ä¿¡æ¯:</strong></p><pre style='background: #f8f9fa; padding: 10px; border-radius: 4px; overflow-x: auto;'>\")\n                if isinstance(test_data['details'], dict):\n                    html += json.dumps(test_data['details'], indent=2, ensure_ascii=False)\n                else:\n                    html += str(test_data['details'])\n                html += \"</pre>\"\n            \n            html += \"</div></div>\"\n        \n        if summary['errors']:\n            html += \"\"\"\n        </div>\n        \n        <div class=\"errors\">\n            <h3>é”™è¯¯æ±‡æ€»</h3>\n\"\"\"\n            for error in summary['errors']:\n                html += f'<div class=\"error-item\">{error}</div>'\n            \n            html += \"</div>\"\n        \n        html += \"\"\"\n    </div>\n</body>\n</html>\n\"\"\"\n        \n        return html\n    \n    def print_summary(self):\n        \"\"\"æ‰“å°æµ‹è¯•æ‘˜è¦\"\"\"\n        summary = self.test_results['summary']\n        success_rate = (summary['passed'] / summary['total'] * 100) if summary['total'] > 0 else 0\n        \n        print(\"\\n\" + \"=\" * 80)\n        print(\"ç³»ç»Ÿæµ‹è¯•æ‘˜è¦\")\n        print(\"=\" * 80)\n        print(f\"æ€»æµ‹è¯•æ•°: {summary['total']}\")\n        print(f\"é€šè¿‡æµ‹è¯•: {summary['passed']}\")\n        print(f\"å¤±è´¥æµ‹è¯•: {summary['failed']}\")\n        print(f\"æˆåŠŸç‡: {success_rate:.1f}%\")\n        print(f\"æ€»è€—æ—¶: {self.test_results['total_duration']:.1f}ç§’\")\n        \n        if summary['errors']:\n            print(\"\\né”™è¯¯åˆ—è¡¨:\")\n            for i, error in enumerate(summary['errors'], 1):\n                print(f\"  {i}. {error}\")\n        \n        print(\"\\n\" + \"=\" * 80)\n        \n        if success_rate >= 90:\n            print(\"ğŸ‰ ç³»ç»Ÿæµ‹è¯•æ•´ä½“é€šè¿‡ï¼\")\n        elif success_rate >= 70:\n            print(\"âš ï¸  ç³»ç»Ÿæµ‹è¯•éƒ¨åˆ†é€šè¿‡ï¼Œéœ€è¦å…³æ³¨å¤±è´¥çš„æµ‹è¯•\")\n        else:\n            print(\"âŒ ç³»ç»Ÿæµ‹è¯•å¤±è´¥è¾ƒå¤šï¼Œéœ€è¦ä¿®å¤é—®é¢˜\")\n        \n        print(\"=\" * 80)\n\n\nasync def main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    # åˆ›å»ºæµ‹è¯•æŠ¥å‘Šç›®å½•\n    report_dir = project_root / 'test_reports'\n    report_dir.mkdir(exist_ok=True)\n    \n    # è¿è¡Œç³»ç»Ÿæµ‹è¯•\n    runner = SystemTestRunner()\n    \n    try:\n        results = await runner.run_all_tests()\n        runner.print_summary()\n        \n        # æ ¹æ®æµ‹è¯•ç»“æœè®¾ç½®é€€å‡ºç \n        success_rate = (results['summary']['passed'] / results['summary']['total'] * 100) if results['summary']['total'] > 0 else 0\n        \n        if success_rate >= 90:\n            sys.exit(0)  # æˆåŠŸ\n        else:\n            sys.exit(1)  # å¤±è´¥\n            \n    except KeyboardInterrupt:\n        logger.info(\"æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­\")\n        sys.exit(130)\n    except Exception as e:\n        logger.error(f\"æµ‹è¯•è¿è¡Œå‡ºé”™: {e}\")\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n